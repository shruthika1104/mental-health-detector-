{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"student_mental_health_dataset.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(\"MentalHealthIssue\", axis=1)\n",
        "y = df[\"MentalHealthIssue\"]\n",
        "\n",
        "# Identify categorical and numeric columns\n",
        "categorical_cols = ['Gender', 'Race', 'Year', 'DisabilityStatus', 'FirstGen', 'Sexuality']\n",
        "numeric_cols = ['KesslerScore', 'PHQScore', 'PC_PTSDScore']\n",
        "\n",
        "# Define transformers\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine transformers into a preprocessor\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply preprocessing and split into train/test sets\n",
        "X_preprocessed = preprocessor.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Shapes for sanity check\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eryW7PzR9_Rv",
        "outputId": "ab6b4b31-24b4-4d53-917d-ca76f96a7c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (400, 18)\n",
            "X_test shape: (100, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup & Libraries\n",
        "\n",
        "!pip install pandas scikit-learn tensorflow joblib --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "import joblib\n",
        "\n",
        "\n",
        "# 2. Load & Clean Data\n",
        "\n",
        "df = pd.read_csv(\"student_mental_health_dataset.csv\")\n",
        "\n",
        "# Optional fallback creation for testing\n",
        "if \"Age\" not in df.columns:\n",
        "    df[\"Age\"] = np.random.randint(17, 30, size=len(df))\n",
        "if \"PHQScore\" in df.columns:\n",
        "    df[\"DepressionScore\"] = df[\"PHQScore\"]\n",
        "if \"AnxietyScore\" not in df.columns:\n",
        "    df[\"AnxietyScore\"] = np.random.uniform(5, 15, size=len(df))\n",
        "if \"PC_PTSDScore\" not in df.columns:\n",
        "    df[\"TraumaScore\"] = np.random.randint(0, 5, size=len(df))\n",
        "else:\n",
        "    df[\"TraumaScore\"] = df[\"PC_PTSDScore\"]\n",
        "\n",
        "# Required columns\n",
        "df = df.rename(columns={\n",
        "    \"Gender\": \"Gender\",\n",
        "    \"DisabilityStatus\": \"Disability\",\n",
        "})\n",
        "\n",
        "required = [\"Gender\", \"Disability\", \"Age\", \"DepressionScore\", \"AnxietyScore\", \"TraumaScore\", \"MentalHealthIssue\"]\n",
        "missing = [col for col in required if col not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns: {missing}\")\n",
        "\n",
        "X = df[[\"Gender\", \"Disability\", \"Age\", \"DepressionScore\", \"AnxietyScore\", \"TraumaScore\"]]\n",
        "y = df[\"MentalHealthIssue\"]\n",
        "\n",
        "# 3. Preprocessing\n",
        "\n",
        "cat_cols = [\"Gender\", \"Disability\"]\n",
        "num_cols = [\"Age\", \"DepressionScore\", \"AnxietyScore\", \"TraumaScore\"]\n",
        "\n",
        "cat_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "num_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"cat\", cat_transformer, cat_cols),\n",
        "    (\"num\", num_transformer, num_cols)\n",
        "])\n",
        "\n",
        "# Apply preprocessing\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save preprocessor\n",
        "joblib.dump(preprocessor, \"scaler.pkl\")\n",
        "\n",
        "\n",
        "# 4. Neural Network Model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(X_processed.shape[1],)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=16, validation_data=(X_test, y_test), verbose=0)\n",
        "\n",
        "\n",
        "# 5. Save Model\n",
        "\n",
        "model.save(\"mental_health_risk_model.h5\")\n",
        "print(\"✅ Trained & saved model and scaler with 6 features.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pMr7saE7JJJ",
        "outputId": "c10da62d-39c7-46da-f6a7-17ab200985b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Trained & saved model and scaler with 6 features.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Train Logistic Regression\n",
        "log_reg_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_reg_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = log_reg_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcEU_dbB-dpw",
        "outputId": "720db7f1-fb34-4d1c-e65c-2d624e75a4ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 71.00%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.43      0.51        35\n",
            "           1       0.74      0.86      0.79        65\n",
            "\n",
            "    accuracy                           0.71       100\n",
            "   macro avg       0.68      0.65      0.65       100\n",
            "weighted avg       0.70      0.71      0.69       100\n",
            "\n",
            "Confusion Matrix:\n",
            "[[15 20]\n",
            " [ 9 56]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Setup early stopping to avoid overfitting\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=50,\n",
        "    batch_size=16,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate on test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Predict and report\n",
        "y_pred_nn = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_nn))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_nn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wdiPms1-1b5",
        "outputId": "38aad666-e075-4e61-9be5-a056bf9dbecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.5061 - loss: 0.7178 - val_accuracy: 0.6250 - val_loss: 0.6212\n",
            "Epoch 2/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6770 - loss: 0.6112 - val_accuracy: 0.6625 - val_loss: 0.5798\n",
            "Epoch 3/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6713 - loss: 0.5868 - val_accuracy: 0.6625 - val_loss: 0.5631\n",
            "Epoch 4/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7267 - loss: 0.5425 - val_accuracy: 0.6750 - val_loss: 0.5570\n",
            "Epoch 5/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7856 - loss: 0.4796 - val_accuracy: 0.7125 - val_loss: 0.5507\n",
            "Epoch 6/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7440 - loss: 0.5243 - val_accuracy: 0.7375 - val_loss: 0.5562\n",
            "Epoch 7/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8071 - loss: 0.4516 - val_accuracy: 0.7375 - val_loss: 0.5537\n",
            "Epoch 8/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7619 - loss: 0.5032 - val_accuracy: 0.7000 - val_loss: 0.5651\n",
            "Epoch 9/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7860 - loss: 0.4582 - val_accuracy: 0.7375 - val_loss: 0.5593\n",
            "Epoch 10/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7805 - loss: 0.4723 - val_accuracy: 0.7375 - val_loss: 0.5559\n",
            "\n",
            "Test Accuracy: 75.00%\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.46      0.56        35\n",
            "           1       0.76      0.91      0.83        65\n",
            "\n",
            "    accuracy                           0.75       100\n",
            "   macro avg       0.74      0.68      0.69       100\n",
            "weighted avg       0.75      0.75      0.73       100\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[16 19]\n",
            " [ 6 59]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save logistic regression model\n",
        "import joblib\n",
        "joblib.dump(log_reg_model, 'logistic_model.pkl')\n",
        "\n",
        "# Save neural network model\n",
        "model.save('neural_net_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M-VzqFc_nBn",
        "outputId": "9fa21fa2-2025-4447-af15-624e3bc8f643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_mental_health(input_data, model_type='logistic'):\n",
        "    if model_type == 'logistic':\n",
        "        model = joblib.load('logistic_model.pkl')\n",
        "        return model.predict(input_data)\n",
        "    elif model_type == 'neural':\n",
        "        from tensorflow.keras.models import load_model\n",
        "        model = load_model('neural_net_model.h5')\n",
        "        prediction = model.predict(input_data)\n",
        "        return (prediction > 0.5).astype(\"int32\")\n"
      ],
      "metadata": {
        "id": "4qTWLU2K_xSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install if not available\n",
        "# !pip install xgboost\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Train an XGBoost classifier\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(f\"XGBoost Accuracy: {accuracy_score(y_test, y_pred_xgb) * 100:.2f}%\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_xgb))\n",
        "\n",
        "# Save the model\n",
        "import joblib\n",
        "joblib.dump(xgb_model, \"xgboost_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeqeBLY2sDj1",
        "outputId": "f6ca6919-0e75-442c-dfcd-5cbd613f224f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Accuracy: 82.00%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.69      0.73        35\n",
            "           1       0.84      0.89      0.87        65\n",
            "\n",
            "    accuracy                           0.82       100\n",
            "   macro avg       0.81      0.79      0.80       100\n",
            "weighted avg       0.82      0.82      0.82       100\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[24 11]\n",
            " [ 7 58]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:49:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['xgboost_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "classifier = XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    max_depth=3,\n",
        "    n_estimators=50,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "2N57enea9uus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-import necessary libraries if needed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "# Define the correct categorical and numerical columns based on the current X\n",
        "cat_cols = [\"Gender\", \"Disability\"]\n",
        "num_cols = [\"Age\", \"DepressionScore\", \"AnxietyScore\", \"TraumaScore\"]\n",
        "\n",
        "# Redefine the preprocessor with the correct columns\n",
        "cat_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "num_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"cat\", cat_transformer, cat_cols),\n",
        "    (\"num\", num_transformer, num_cols)\n",
        "])\n",
        "\n",
        "# Define the classifier (using XGBoost as an example based on subsequent code)\n",
        "classifier = xgb.XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1\n",
        ")\n",
        "\n",
        "# Create the pipeline including preprocessing and the classifier\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', classifier)\n",
        "])\n",
        "\n",
        "# Perform cross-validation using the newly defined pipeline\n",
        "cv_scores = cross_val_score(pipeline, X, y, cv=5)\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5iqOo4EuBAu",
        "outputId": "bffa8872-0acf-48bd-85c0-23bb61cfcbeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:49:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:49:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:49:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.82 0.79 0.76 0.82 0.79]\n",
            "Mean CV accuracy: 0.796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:49:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:49:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install pandas scikit-learn xgboost matplotlib --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv(\"student_mental_health_dataset.csv\")\n",
        "\n",
        "# Rename columns to match pipeline expectations\n",
        "df = df.rename(columns={\n",
        "    \"DisabilityStatus\": \"Disability\",\n",
        "    \"PHQScore\": \"DepressionScore\",\n",
        "    \"PC_PTSDScore\": \"TraumaScore\"\n",
        "})\n",
        "\n",
        "# Add missing columns if needed (for testing only)\n",
        "if \"AnxietyScore\" not in df.columns:\n",
        "    df[\"AnxietyScore\"] = np.random.uniform(5, 15, size=len(df))\n",
        "if \"Age\" not in df.columns:\n",
        "    df[\"Age\"] = np.random.randint(17, 30, size=len(df))\n",
        "\n",
        "# Check required columns\n",
        "required = [\"Gender\", \"Disability\", \"Age\", \"DepressionScore\", \"AnxietyScore\", \"TraumaScore\", \"MentalHealthIssue\"]\n",
        "missing = [col for col in required if col not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"❌ Missing required columns: {missing}\")\n",
        "\n",
        "# Define features and target\n",
        "X = df[[\"Gender\", \"Disability\", \"Age\", \"DepressionScore\", \"AnxietyScore\", \"TraumaScore\"]]\n",
        "y = df[\"MentalHealthIssue\"]\n",
        "\n",
        "\n",
        "cat_cols = [\"Gender\", \"Disability\"]\n",
        "num_cols = [\"Age\", \"DepressionScore\", \"AnxietyScore\", \"TraumaScore\"]\n",
        "\n",
        "cat_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "num_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"cat\", cat_transformer, cat_cols),\n",
        "    (\"num\", num_transformer, num_cols)\n",
        "])\n",
        "\n",
        "# ========================\n",
        "# 5. XGBoost Classifier\n",
        "# ========================\n",
        "classifier = xgb.XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', classifier)\n",
        "])\n",
        "\n",
        "\n",
        "# 6. Cross-validation\n",
        "\n",
        "cv_scores = cross_val_score(pipeline, X, y, cv=5)\n",
        "print(\"✅ Cross-validation scores:\", cv_scores)\n",
        "print(\"✅ Mean CV accuracy:\", np.mean(cv_scores))\n",
        "\n",
        "# 7. Fit Final Model\n",
        "\n",
        "pipeline.fit(X, y)\n",
        "\n",
        "# Predict on full data (or split if you want)\n",
        "y_pred = pipeline.predict(X)\n",
        "\n",
        "print(\"\\n✅ Accuracy:\", accuracy_score(y, y_pred))\n",
        "print(\"\\n📊 Classification Report:\\n\", classification_report(y, y_pred))\n",
        "print(\"\\n🧮 Confusion Matrix:\\n\", confusion_matrix(y, y_pred))\n",
        "\n",
        "# 8. Feature Importance Plot\n",
        "\n",
        "# Get feature names\n",
        "encoder = pipeline.named_steps[\"preprocessor\"].named_transformers_[\"cat\"].named_steps[\"encoder\"]\n",
        "encoded_cat_names = encoder.get_feature_names_out(cat_cols).tolist()\n",
        "feature_names = encoded_cat_names + num_cols\n",
        "\n",
        "# Get importances\n",
        "xgb_model = pipeline.named_steps[\"classifier\"]\n",
        "importances = xgb_model.feature_importances_\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8-ekugW856P",
        "outputId": "18addc2c-fdff-4373-b769-872fb3f92138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:49:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:49:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:49:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:49:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:49:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:49:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cross-validation scores: [0.85 0.78 0.78 0.84 0.83]\n",
            "✅ Mean CV accuracy: 0.8160000000000001\n",
            "\n",
            "✅ Accuracy: 0.986\n",
            "\n",
            "📊 Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98       185\n",
            "           1       0.99      0.98      0.99       315\n",
            "\n",
            "    accuracy                           0.99       500\n",
            "   macro avg       0.98      0.99      0.99       500\n",
            "weighted avg       0.99      0.99      0.99       500\n",
            "\n",
            "\n",
            "🧮 Confusion Matrix:\n",
            " [[183   2]\n",
            " [  5 310]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install scikeras[tensorflow] xgboost scikit-learn pandas --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"student_mental_health_dataset.csv\")\n",
        "\n",
        "df['is_Hispanic_Woman'] = ((df['Gender'] == 'Female') & (df['Race'] == 'Hispanic')).astype(int)\n",
        "df['is_Disabled_LGBTQA'] = ((df['DisabilityStatus'] == 'Yes') & (df['Sexuality'] != 'Heterosexual')).astype(int)\n",
        "df['is_FirstGen_Woman'] = ((df['Gender'] == 'Female') & (df['FirstGen'] == 'Yes')).astype(int)\n",
        "\n",
        "features = ['Gender', 'Race', 'Year', 'DisabilityStatus', 'FirstGen', 'Sexuality',\n",
        "            'is_Hispanic_Woman', 'is_Disabled_LGBTQA', 'is_FirstGen_Woman']\n",
        "target = 'MentalHealthIssue'\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "cat_cols = ['Gender', 'Race', 'Year', 'DisabilityStatus', 'FirstGen', 'Sexuality']\n",
        "num_cols = ['is_Hispanic_Woman', 'is_Disabled_LGBTQA', 'is_FirstGen_Woman']\n",
        "\n",
        "cat_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "num_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', num_transformer, num_cols),\n",
        "    ('cat', cat_transformer, cat_cols)\n",
        "])\n",
        "\n",
        "# Apply preprocessing\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# 4. Logistic Regression\n",
        "\n",
        "log_model = LogisticRegression(max_iter=1000)\n",
        "log_model.fit(X_train, y_train)\n",
        "log_pred = log_model.predict(X_test)\n",
        "print(\"🔵 Logistic Regression\")\n",
        "print(classification_report(y_test, log_pred))\n",
        "\n",
        "\n",
        "# 5. XGBoost\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "print(\"🟢 XGBoost\")\n",
        "print(classification_report(y_test, xgb_pred))\n",
        "\n",
        "\n",
        "# 6. Neural Network\n",
        "\n",
        "def create_nn_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(16, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "nn_model = KerasClassifier(model=create_nn_model, epochs=20, batch_size=16, verbose=0)\n",
        "nn_model.fit(X_train, y_train)\n",
        "nn_pred = nn_model.predict(X_test)\n",
        "print(\"🔴 Neural Network\")\n",
        "print(classification_report(y_test, nn_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpQohhW9xrVU",
        "outputId": "e8102cac-8545-4e4a-e460-988d22ce510a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔵 Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.12      0.03      0.05        35\n",
            "           1       0.63      0.89      0.74        65\n",
            "\n",
            "    accuracy                           0.59       100\n",
            "   macro avg       0.38      0.46      0.39       100\n",
            "weighted avg       0.45      0.59      0.50       100\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:49:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🟢 XGBoost\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.17      0.24        35\n",
            "           1       0.66      0.86      0.75        65\n",
            "\n",
            "    accuracy                           0.62       100\n",
            "   macro avg       0.53      0.52      0.49       100\n",
            "weighted avg       0.57      0.62      0.57       100\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔴 Neural Network\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        35\n",
            "           1       0.65      1.00      0.79        65\n",
            "\n",
            "    accuracy                           0.65       100\n",
            "   macro avg       0.33      0.50      0.39       100\n",
            "weighted avg       0.42      0.65      0.51       100\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predictions to NumPy arrays\n",
        "log_np = np.array(log_pred)\n",
        "xgb_np = np.array(xgb_pred)\n",
        "nn_np = np.array(nn_pred)\n",
        "\n",
        "# Majority vote\n",
        "combined = (log_np + xgb_np + nn_np) >= 2\n",
        "combined = combined.astype(int)\n",
        "\n",
        "print(\"🟣 Ensemble Majority Vote\")\n",
        "print(classification_report(y_test, combined))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwfgbkOuxsVq",
        "outputId": "5c8c8afe-ab9b-4a3a-c3eb-2575db13b992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🟣 Ensemble Majority Vote\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        35\n",
            "           1       0.63      0.92      0.75        65\n",
            "\n",
            "    accuracy                           0.60       100\n",
            "   macro avg       0.32      0.46      0.38       100\n",
            "weighted avg       0.41      0.60      0.49       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(\"student_mental_health_dataset.csv\")\n",
        "\n",
        "# Rename columns to match expected pipeline input\n",
        "df = df.rename(columns={\n",
        "    \"DisabilityStatus\": \"Disability\",\n",
        "    \"PHQScore\": \"DepressionScore\",\n",
        "    \"PC_PTSDScore\": \"TraumaScore\"\n",
        "})\n",
        "\n",
        "# Add missing columns if needed (for testing only)\n",
        "if \"AnxietyScore\" not in df.columns:\n",
        "    df[\"AnxietyScore\"] = np.random.uniform(5, 15, size=len(df))\n",
        "if \"Age\" not in df.columns:\n",
        "    df[\"Age\"] = np.random.randint(17, 30, size=len(df))\n",
        "\n",
        "# Check for required columns\n",
        "required = [\"Gender\", \"Disability\", \"Age\", \"DepressionScore\", \"AnxietyScore\", \"TraumaScore\", \"MentalHealthIssue\"]\n",
        "missing = [col for col in required if col not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns: {missing}\")\n",
        "\n",
        "# Define features and target as a DataFrame (not numpy array!)\n",
        "X = df[[\"Gender\", \"Disability\", \"Age\", \"DepressionScore\", \"AnxietyScore\", \"TraumaScore\"]]\n",
        "y = df[\"MentalHealthIssue\"]\n"
      ],
      "metadata": {
        "id": "VxsTLMXI9W5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Column setup\n",
        "cat_cols = [\"Gender\", \"Disability\"]\n",
        "num_cols = [\"Age\", \"DepressionScore\", \"AnxietyScore\", \"TraumaScore\"]\n",
        "\n",
        "# Preprocessor\n",
        "cat_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "num_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"cat\", cat_transformer, cat_cols),\n",
        "    (\"num\", num_transformer, num_cols)\n",
        "])\n",
        "\n",
        "# Model pipeline\n",
        "pipeline = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
        "])\n",
        "\n",
        "# ✅ Now this works:\n",
        "cv_scores = cross_val_score(pipeline, X, y, cv=5)\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bp9T-0yr9X_w",
        "outputId": "f55084df-855e-450b-caee-eb948cee633b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:50:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:50:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:50:12] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:50:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:50:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.79 0.76 0.73 0.79 0.79]\n",
            "Mean CV accuracy: 0.772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.fit(X, y) # It's good practice to fit the pipeline before saving if you intend to use it for predictions later.\n",
        "joblib.dump(pipeline, \"mental_health_xgb_pipeline.pkl\")\n",
        "print(\"✅ Saved a FITTED pipeline.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tsjFTqkJhKD",
        "outputId": "3f9752a8-8c6f-4733-b5e2-8d0bfefcb23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:50:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved a FITTED pipeline.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.fit(X, y)\n",
        "joblib.dump(pipeline, \"mental_health_xgb_pipeline.pkl\")\n",
        "print(\"✅ Saved a FITTED pipeline.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDajcc0i_g_C",
        "outputId": "1d53723f-2666-4d81-a670-0cece5a7eabc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:50:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved a FITTED pipeline.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = joblib.load(\"mental_health_xgb_pipeline.pkl\")\n"
      ],
      "metadata": {
        "id": "I-WwTXn3_iEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the whole preprocessing + model pipeline\n",
        "joblib.dump(pipeline, \"mental_health_xgb_pipeline.pkl\")\n",
        "print(\"✅ Saved: mental_health_xgb_pipeline.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqUM3k4g9kv6",
        "outputId": "ca94767e-366f-45af-c248-b3bb246b7e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: mental_health_xgb_pipeline.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWn8Vdka-2OT",
        "outputId": "5327728f-68bd-40ec-9088-f5935ee07025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m113.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Load trained pipeline\n",
        "pipeline = joblib.load(\"mental_health_xgb_pipeline.pkl\")\n",
        "\n",
        "# Prediction function\n",
        "def predict_risk(gender, disability, age, depression, anxiety, trauma):\n",
        "    try:\n",
        "        # Construct DataFrame for a single row\n",
        "        input_df = pd.DataFrame([{\n",
        "            \"Gender\": gender,\n",
        "            \"Disability\": disability,\n",
        "            \"Age\": age,\n",
        "            \"DepressionScore\": depression,\n",
        "            \"AnxietyScore\": anxiety,\n",
        "            \"TraumaScore\": trauma\n",
        "        }])\n",
        "\n",
        "        # Predict using pipeline\n",
        "        pred = pipeline.predict(input_df)[0]\n",
        "        prob = pipeline.predict_proba(input_df)[0][1]\n",
        "\n",
        "        result = \"🔴 High Risk\" if pred == 1 else \"🟢 Low Risk\"\n",
        "        return f\"{result} ({prob:.2%} chance)\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"⚠️ Error: {str(e)}\"\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as app:\n",
        "    gr.Markdown(\"<h1 style='text-align:center'>🧠 Mental Health Risk Prediction</h1>\")\n",
        "    gr.Markdown(\"<p style='text-align:center'>This app uses an XGBoost model to assess mental health risk based on your inputs.</p>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gender = gr.Radio([\"Female\", \"Male\", \"Other\"], label=\"Gender\")\n",
        "            disability = gr.Radio([\"Yes\", \"No\"], label=\"Disability Status\")\n",
        "            age = gr.Slider(17, 30, value=20, step=1, label=\"Age\")\n",
        "            depression = gr.Slider(0, 27, step=0.1, value=10, label=\"Depression Score\")\n",
        "            anxiety = gr.Slider(0, 21, step=0.1, value=9, label=\"Anxiety Score\")\n",
        "            trauma = gr.Slider(0, 5, step=1, value=2, label=\"Trauma Score\")\n",
        "            submit = gr.Button(\"🚀 Submit\")\n",
        "\n",
        "        with gr.Column():\n",
        "            output = gr.Textbox(label=\"Mental Health Risk Level\", lines=2)\n",
        "\n",
        "    submit.click(predict_risk, inputs=[gender, disability, age, depression, anxiety, trauma], outputs=output)\n",
        "\n",
        "# Launch the app\n",
        "app.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "X6lCupo89tCE",
        "outputId": "c2cae4df-f271-4e44-a0de-733b60ff2535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://cac24a644a43719ffd.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cac24a644a43719ffd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}